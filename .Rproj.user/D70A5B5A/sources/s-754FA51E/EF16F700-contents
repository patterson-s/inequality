---
title: "hw2"
author: "Scott Patterson"
date: '2020-01-22'
output: 
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 1) 

# a) List all of the nodes in Graph A.

The nodes in graph A are Ua, Ub, Z, X, Y. Ua and Ub are hypothesized but unobserved variables while the other variables are observed. 

# b) List all of the edges in Graph A.
Edges represent causal effects. The edges between observed variables are Z-X and X-Y. The edges between unobserved and observed variables are Ua-Z, Ub-X, and Ub-Y. 

# c) List each path terminating at Y that exists in Graph A, and repeat for Graph B. Interpret the differences between graphs.

For graph A: 
Z-X-Y, where Z=f(U_z), X=f(Z,U_x), and Y=f(U_y)

For graph B: 
XD-X-Y, where Y=f(U_b)

Graph A is a directed path from Z to X to Y. Variable Z may may be determined by the unobserved variable U_a. Variables X and Y may be mutually determined by the unobersved variable U_b. In other words, U_b is potentially a confounder on the relationship between X and Y. 

Graph B adds an intervening variable, X_D, which influences the variable X, which in turn affects the variable Y. Variable Y may still be dependent on the unobserved variable U_b. 

The difference between the two graphs is that in graph B, the variable X is affected by the intervening variable X_D. This intervention removes the variable Z from the causal path. The intervening variable is also observed to influence X, ending X's dependence on the parent U_b. Variable Y, however, may still be dependent on U_b. 

# d) Explain the relationship between X_D and X in Graph B.
The variable X_D is a treatment on variable X. X_D is a do operator that distinguishes between two causal states of X. 

# e) Is the relationship between X and Y identified in Graph B? Why?
The relationship between X and Y is NOT identified because U_b confounds Y. If X and Y were mutually dependent on U_b, a consistent unbiased estimate of the relationship could be made. However, since only Y is dependent on U_b, the relationship is not identified. 

# f) Assuming ... write down the ATE of X on Y for Causal Graph B using the notation introduced in class.

E[Y | do(X_1)] - E[Y | do(X_0)]




# 2

```{r Library}
pacman::p_load(readr, dplyr, ggplot2, reshape2, knitr, sandwich)

```


```{r Read Data}

olken <- read_csv("data copy/olken.csv")
glimpse(olken)

```
#a)
```{r Balance Table }

# Select covariates to balance on ----
balance_variables <- olken %>%
  dplyr::select(head.edu, mosques, pct.poor, total.budget)

# Calculate mean and SD by treatment status ----
balance.mean <- aggregate(balance_variables, by=list(olken$treat.invite),
function(x) mean(x, na.rm =T))

balance.sd <- aggregate(balance_variables, by=list(olken$treat.invite),
function(x) sd(x, na.rm =T))

# Difference in means for each covariate ----

dif.mean <- vector()
for (i in 1:5){
  dif.mean[i] <- balance.mean[balance.mean$Group.1==1 , i] - balance.mean[balance.mean$Group.1==0 , i]
}

dif.mean[2:5] # I used [2:5] to exclude treatment status


# balance table ----

bal <- rbind(balance.mean, balance.sd, dif.mean)

bal_1 <- t(bal)

colnames(bal_1) = c("Control_Mean", "Treat_Mean", "Control_SD",
"Treat_SD", "Diff_Means" )

balance_table <- bal_1[-1,]

rm(bal,bal_1)

# Test hypothesis that difference in means between treatment conditions is 0 ----

ttest.pval <- vector()
for(i in 1:4){
 ttest.pval[i] <- (t.test(balance_table[i,1:2]))$p.value
}

t_1 <- (t.test(balance_table[1,1:2]))$p.value


# Add ttest.pval to balance table ----

balance_table <- cbind(balance_table,ttest.pval)

# Balance table as tibble ----

as_tibble(balance_table) %>%
  mutate(Covariate = c("head.edu","mosques","pct.poor","total.budget")) %>%
  dplyr::select(Covariate, everything()) %>%
  kable(type = "text")

```


#b) For each covariate, plot its distributions under treatment and control (side by side or overlaying).

```{r Treatment v. Control Plot}

# Plot dataframe ----
tc.plot <- olken %>%
  dplyr::select(head.edu, mosques, pct.poor, total.budget,treat.invite) %>%
  mutate(id = row_number(),
         treat.invite = factor(treat.invite,
                               labels = c("Control","Treatment")))

# Melt for plotting ----
tc.melt <- melt(tc.plot, id.vars = c("id", "treat.invite"))

# Density Plot ----
tc.density <- ggplot(tc.melt, aes(x = value, fill = treat.invite)) +
  geom_density() +
  facet_wrap(~ variable, scales="free") +
  theme_classic() +
  theme(legend.position="bottom")

tc.density

```

# c) Do villages in each condition appear similar in the pre-treatment covariates? Explain the importance of checking balance in a randomized
experiment and the result you typically expect to find.

There are slight differences in the pre-treatment condition of villages. The control group has greater variance than the treatment group, as seen in the faceted plots for `tc.density`. Overall, the shapes are similar but not exactly the same. 

It is important to check for balance in a randomized experiment to avoid producing bias. It is possible that samples can be imbalanced through randomization errors or small size. There are post-randomization adjustment processes that can improve model efficiency, but these processes can also produce their own types of bias. Ensuring up-front balance pre-empts the need for post-randomization adjustments. 

# d) Regress treatment on the pre-treatment covariates and report the p-value of an omnibus F-test. What do you conclude from the results?

```{r Pre-treatment Covariate Adjustment}

# Regression ----

pre_balance <- lm(treat.invite ~ head.edu + mosques + pct.poor + total.budget,
                  data = olken)

summary(pre_balance)


```
The F-statistic (or the p-value of the F-test) is 0.1906. 


# e) Using the difference-in-means estimator, estimate the average treatment effect and its standard error.

```{r Average Treatment Effect}
# Average Treatment Effect (difference in means) ----

ybar <- tapply(olken$pct.missing,
               list("treated" = olken$treat.invite),
               function(x) mean(x, na.rm = T))

ATE <- ybar['1'] - ybar['0']
ATE

# Standard Error ----

se.ATE <- function(y, tx){
  y1 = y[tx==1]
  y0 = y[tx == 0]
  n1 = length(y1)
  n0 = length(y0)
  sqrt(((var(y1)/n1 + var(y0)/n0)))
}

se.ATE(olken$pct.missing,olken$treat.invite)



```

# f) Now estimate the average treatment effect and its standard error using a bivariate regression of outcomes on treatment. Are the results different from those in e)? Make the changes necessary for them to match exactly, and explain your method.

```{r Bivariate OLS}

# Bivariate Model ----
bi.ols <- lm(pct.missing ~ treat.invite, data=olken)
summary(bi.ols)$coefficients


```

The average treatment effects for the bivariate OLS and the difference in means estimator is equivalent. The standard error estimates are close but they differ at the 3rd decimal point. 

```{r Robust SE}

bi.robust.se <- sqrt(diag(vcovHC(bi.ols, type='HC2')))
bi.robust.se

```

The standard errors are equivalent when applying robust standard errors. This can be done through the `sandwich` package. 



# g) 
Reestimate the average treatment effect using a regression specification that includes pretreatment covariates (additively and linearly). Report your estimates of the treatment effect and its standard error. Do you expect them to differ from the difference-in-means estimates, and do they? Explain.

```{r}



```



# h) Estimate the ATE for villages with more than half of households below the poverty line, and then do the same for villages with less than half of households below the poverty line. Estimate the standard error of the difference in treatment effects and test the null hypothesis that there is no difference between them. What do you conclude?

